---
title: "Panel Counterfactual Estimators for Policy Shocks under Latent Trends"
subtitle: "A Simulation Study Motivated by the Italy–Libya MoU"
author: "Giorgio Coppola"
date: today

format:
  pdf:
    documentclass: article
    classoption: [10pt, a4paper]
    geometry:
      - margin=1in
      - headheight=13pt
    fontsize: 10pt
    linestretch: 1
    number-sections: true
    toc: false
    toc-depth: 2
    toc-title: "Contents"
    colorlinks: true
    linkcolor: NavyBlue
    urlcolor: NavyBlue
    citecolor: NavyBlue
    pdf-engine: pdflatex
    include-in-header:
      text: |
        \usepackage{fancyhdr}
        \pagestyle{fancy}
        \fancyhead[L]{\small Panel Counterfactual Estimators}
        \fancyhead[R]{\small\thepage}
        \fancyfoot[C]{}
        \usepackage{booktabs}
        \usepackage{float}
        \floatplacement{figure}{H}
        \floatplacement{table}{H}

  html:
    theme: [cosmo, custom.scss]
    toc: true
    toc-location: left
    toc-depth: 3
    number-sections: true
    smooth-scroll: true
    link-external-icon: true
    anchor-sections: true
    code-fold: true
    code-tools: true
    highlight-style: github
    html-math-method: katex
    fig-cap-location: bottom
    tbl-cap-location: top
    citations-hover: true
    title-block-banner: "#0b1f3a"
    title-block-banner-color: white

execute:
  echo: false
  warning: false
  message: false

bibliography: references.bib
---

```{r setup}
library(tidyverse)
library(knitr)
library(kableExtra)

knitr::opts_chunk$set(
  fig.align = "center",
  out.width = "85%"
)
```

# Introduction

Difference-in-differences (DiD) is the classic method for evaluating policy interventions in observational panel data. The "canonical" estimator for many periods/multiple groups panels is the two-way fixed effects (TWFE), which relies on the parallel trends assumption: absent treatment, treated and control units would have followed parallel trajectories. However, this assumption often fails in practice when units exhibit complex or heterogeneous trends driven by unobserved time-varying factors. Recent methodological advances propose estimators robust to violations of parallel trends by modeling low-rank latent factor structures. This project compares four panel counterfactual estimators for a spatial policy shock: TWFE, Matrix Completion (MC), Synthetic Difference-in-Differences (SDID), and the Triply Robust Panel Estimator (TROP). Using Monte Carlo simulations calibrated to the Missing Migrants Project data for the Central Mediterranean from IOM, this project assesses each method's bias, coverage, and power under varying-strength interactive fixed effects. The motivating application is the Italy–Libya Memorandum of Understanding (which came into effect in February 2017), which led to the externalization of border control from the Italian Coast Guard to Libyan Coast Guard patrols, starting in May 2017. This agreement has been, and still is, at the center of the Italian and European political and academic debate, questioning its effectiveness, as well as its ethical premises, as externalization practices are often accompanied by the criminalization of NGO's SAR operations, and therefore they are associated with increased mortality and dangerousness of the migratory routes. If this policy landscape serves as a backdrop, the research question of this project is methodological: when latent trends conflict with parallel trends (as is often the case in these kinds of settings), which estimator should practitioners use?

# Background

Consider a balanced panel with units $i = 1, \ldots, N$ observed over periods $t = 1, \ldots, T$. The potential outcomes framework defines $Y_{it}(0)$ and $Y_{it}(1)$ as outcomes under control and treatment. A binary treatment indicator $D_{it} \in \{0, 1\}$ determines the observed outcome:
$$
Y_{it} = Y_{it}(0) + D_{it} \cdot \tau_{it}, \quad \text{where } \tau_{it} = Y_{it}(1) - Y_{it}(0).
$$

In a canonical 2×2 DiD design, treatment turns on for a subset of units after period $T_0$. TWFE estimates the average treatment effect on the treated (ATT) by regressing:
$$
Y_{it} = \alpha_i + \lambda_t + \delta D_{it} + \varepsilon_{it},
$$
where $\alpha_i$ and $\lambda_t$ are unit and time fixed effects. This is unbiased for ATT if parallel trends holds: $E[Y_{it}(0) | i \in \text{treated}] - E[Y_{it}(0) | i \in \text{control}] = c$ (constant over time).

Parallel trends fails when units follow heterogeneous trends. A common generalization is the interactive fixed effects model (Bai 2009):
$$
Y_{it}(0) = \alpha_i + \lambda_t + \lambda_i' F_t + \varepsilon_{it},
$$
where $\lambda_i \in \mathbb{R}^K$ are unit-specific loadings and $F_t \in \mathbb{R}^K$ are common time factors. Critically, TWFE is biased when factor loadings $\lambda_i$ are correlated with treatment assignment, that is, when treated units have systematically different loadings than control units, creating differential trends.

Recent work proposes estimators that accommodate latent trends:

- Matrix Completion (MC): Models the untreated potential outcome matrix $Y(0)$ as low-rank, using singular value decomposition (SVD) to impute counterfactuals for treated observations (Athey et al. 2021).

- Synthetic Difference-in-Differences (SDID): Constructs synthetic controls using pre-period matching, combining unit weights (to match treated units' pre-period means) with time weights (to emphasize recent pre-periods), then applies a DiD-style contrast (Arkhangelsky et al. 2021).

- Triply Robust Panel Estimator (TROP): Combines unit weights, time weights, and regression adjustment. Consistent if any one of three conditions holds: unit weights balance loadings, time weights balance factors, or the regression adjustment is correctly specified (Athey et al. 2025).

# Methods

A spatial panel of $N = 56$ grid cells (1° × 1° resolution) in the Central Mediterranean is simulated, observed monthly from April 2015 to February 2018 ($T = 35$). The treatment is defined as:
$$
D_{it} = 1\{\text{distance}_i \leq 200\text{ km from Tripoli}\} \times 1\{t \geq \text{May 2017}\}.
$$

The outcome $Y_{it}^{\text{any}} = 1\{\text{deaths observed in cell } i \text{, month } t > 0\}$ is a binary deadly-event indicator. Deaths are generated hierarchically as (1) Exposure, where migration attempts $M_{it} \sim \text{Poisson}(\mu_{it})$, where $\mu_{it}$ includes seasonality and latent trends; (2) mortality, where each attempt has per-attempt mortality probability $p_{it}$ following a logistic model with interactive fixed effects, and (3) deaths, $\text{Deaths}_{it} \sim \text{Binomial}(M_{it}, p_{it})$.  The DGP is calibrated to match pre-period deadly-event rates (~6.6%) from IOM Missing Migrants data. I vary `factor_strength` (0 = no latent factors, 0.5 = moderate, 0.9 = strong), `loading_correlation` (correlation between loadings and treatment assignment), and the treatment effect size `delta` on the log-odds scale. All estimators target the ATT for the binary outcome $Y^{\text{any}}$, treating it as continuous (linear probability model) for comparability. TWFE uses `feols(Y ~ D | unit_id + time_id)` with cluster-robust SEs. Matrix Completion uses the `fect` package with SVD-based imputation and nuclear norm regularization, with bootstrap SEs. SynthDID uses the `synthdid` package with jackknife SEs for valid inference. TROP uses exponential kernel weights for units and time with low-rank regression adjustment, with bootstrap SEs.

# Simulation Design

Two experiments are conducted:

1. Power analysis: Fix `factor_strength = 0.5` and `loading_correlation = 0.5` (moderate latent trends correlated with treatment) and vary treatment effect $\delta \in \{0, 0.2, 0.4, 0.6, 0.8\}$. Run 200 replications per $\delta$ value.

2. Scenario analysis: Fix $\delta = 0.6$ and vary DGP features (parallel trends, moderate/strong latent trends, underreporting, short-lived effect). Run 100 replications per scenario.

Performance metrics: bias, RMSE, 95% CI coverage (nominal target), and rejection rate (power/size at $\alpha = 0.05$).

# Results

## One-dataset estimation example

@tbl-example-estimates reports estimates on the calibrated DGP for a single simulated dataset (balanced 56×35 panel). This “one draw” is useful to see what an applied analyst would face in practice, but it can be noisy when outcomes are sparse.

```{r}
#| label: tbl-example-estimates
#| tbl-cap: 'One-dataset estimates (calibrated DGP; outcome $Y_{it} = 1\{\text{deaths}>0\}$).'
#| echo: false
library(tidyverse)

ex <- readr::read_csv("../output/example_estimates.csv", show_col_types = FALSE)

ex %>%
  select(method, estimate, se, true_att, bias, p_value) %>%
  mutate(
    across(c(estimate, se, true_att, bias), ~ round(.x, 3)),
    p_value = signif(p_value, 3)
  ) %>%
  knitr::kable()
```

In this example: TWFE and Matrix Completion (via `fect`) coincide almost exactly (same point estimate, slightly different SEs). This indicates that the `fect` MC fit is effectively behaving like a two-way fixed effects model in this design (e.g., selecting few/no latent factors), so it should not be interpreted as a meaningfully different estimator here. I could not find evidence that MC is delivering a low-rank robustness correction, but this should be better investigated. TROP is closer to the true ATT in this draw and its confidence interval covers the true effect. SynthDiD is imprecise (large SE) and does not reject in this draw; its point estimate is below the true ATT.


## Power Analysis

@fig-power and @tbl-power-summary summarize Monte Carlo performance across treatment strengths (log-odds shifts) under the calibrated DGP.

```{r}
#| label: fig-power
#| fig-cap: "Rejection rates (power/size) across treatment strengths under the calibrated DGP."
#| echo: false
knitr::include_graphics("../figures/power_curves.pdf")
```

Key takeaways from the calibrated power study include that size control is imperfect for some methods. When $\delta=0$ (no effect), TWFE is close to nominal size, whereas Matrix Completion (via `fect`) and SynthDiD reject too often, indicating inflated Type I error. TROP is closest to nominal size and exhibits the best coverage at $\delta=0$. Matrix Completion (via `fect`) does not improve point-estimation accuracy relative to TWFE in this design. Across values of $\delta$, MC and TWFE display essentially the same average bias and RMSE; differences mainly arise in inference (standard errors, coverage, and power), rather than in point estimates. Overall, the results reflect a bias–variance tradeoff. TROP behaves more conservatively, with higher coverage and lower rejection rates, while SynthDiD is typically less powerful and tends to attenuate estimated effects in this binary-outcome setting.

```{r}
#| label: fig-bias
#| fig-cap: "Bias as a function of treatment strength under the calibrated DGP."
#| echo: false
knitr::include_graphics("../figures/bias_curve.pdf")
```

```{r}
#| label: fig-coverage
#| fig-cap: "Coverage of nominal 95% intervals under the calibrated DGP."
#| echo: false
knitr::include_graphics("../figures/coverage_curve.pdf")
```

```{r}
#| label: tbl-power-summary
#| tbl-cap: 'Monte Carlo summary at a moderate effect size ($\delta = 0.8$).'
#| echo: false
library(tidyverse)

ps <- readr::read_csv("../output/power_summary.csv", show_col_types = FALSE)

ps %>%
  filter(delta == 0.8) %>%
  select(method, true_att, mean_estimate, bias, rmse, coverage, rejection_rate) %>%
  mutate(
    across(c(true_att, mean_estimate, bias, rmse, coverage, rejection_rate), ~ round(.x, 3))
  ) %>%
  arrange(desc(rejection_rate)) %>%
  knitr::kable()
```

## Scenario Analysis

The scenario analysis varies the strength of latent trends, measurement error, and whether the treatment effect is transient (May–June only). @tbl-scenario summarizes performance.

```{r}
#| label: tbl-scenario
#| tbl-cap: "Scenario analysis (mean performance across simulations)."
#| echo: false
library(tidyverse)

sc <- readr::read_csv("../output/scenario_summary.csv", show_col_types = FALSE)

sc %>%
  select(scenario, method, true_att, bias, rmse, coverage, power) %>%
  mutate(
    across(c(true_att, bias, rmse, coverage, power), ~ round(.x, 3))
  ) %>%
  arrange(scenario, method) %>%
  knitr::kable()
```

The main qualitative patterns are: in persistent-effect scenarios, TWFE and Matrix Completion behave similarly in terms of bias and RMSE, but Matrix Completion tends to undercover more and can appear “more powerful” largely because it reports smaller standard errors. In the short-lived effect scenario, Matrix Completion can break down sharply, with large bias and severe under-coverage, which is consistent with a mismatch between the estimator’s implicit treatment structure and a transient policy shock. In that scenario, TROP and SynthDiD remain much more stable, albeit with lower power. These simulations are designed to be application-motivated rather than “best case” for any method: sparse binary outcomes, interactive fixed effects with loadings correlated with treatment, and a limited number of treated units. The most important empirical takeaway from the current implementation is that Matrix Completion via `fect` is not acting like a distinct low-rank imputation estimator in this design. In both the one-dataset illustration and the Monte Carlo summaries, its point estimates track TWFE extremely closely. When this happens, MC should be interpreted as “TWFE with different uncertainty quantification,” not as a robustness improvement.

# Future Directions

Several extensions merit investigation:

1. SDID for discrete outcomes: Develop variants of SDID with link functions (logit, Poisson) appropriate for binary or count data, or test performance with continuous transformations like $Y_{\text{ihs}}$.

2. Heterogeneous treatment effects: Current simulations assume constant $\delta$ across treated units. Realistic settings often exhibit effect heterogeneity by distance or time.

3. Optimal rank selection: Investigate data-driven methods for choosing the number of latent factors in MC and TROP (e.g., cross-validation, information criteria).

4. Inference under interference: Spatial settings may exhibit spillover effects between adjacent cells, violating SUTVA.

